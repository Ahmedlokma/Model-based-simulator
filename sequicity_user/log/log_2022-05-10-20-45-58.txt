INFO:root:cuda_device : 4
eos_m_token : EOS_M
beam_len_bonus : 0.5
mode : unknown
m : TSD
prev_z_method : separate
dataset : usr
seed : 0
vocab_size : 8000
embedding_size : 50
hidden_size : 50
lr : 0.003
lr_decay : 0.5
layer_num : 1
z_length : 16
max_ts : 50
early_stop_count : 5
cuda : True
degree_size : 1
split : (9, 1, 1)
root_dir : /Users/ahmedlokma/Desktop/user-simulator-master/sequicity_user
model_path : /Users/ahmedlokma/Desktop/user-simulator-master/sequicity_user/models/trainmodel.pkl
result_path : /Users/ahmedlokma/Desktop/user-simulator-master/sequicity_user/results/trainmodel.csv
vocab_path : /Users/ahmedlokma/Desktop/user-simulator-master/sequicity_user/vocab/vocab-trainmodel.pkl
data : /Users/ahmedlokma/Desktop/user-simulator-master/data/multiwoz-master/data/multi-woz/example2.json
entity : /Users/ahmedlokma/Desktop/user-simulator-master/data/multiwoz-master/data/multi-woz/example.json
db : /Users/ahmedlokma/Desktop/user-simulator-master/data/multiwoz-master/data/multi-woz/trainmodel.json
glove_path : ../sequicity/data/glove/glove.6B.50d.txt
batch_size : 32
dropout_rate : 0.5
epoch_num : 100
rl_epoch_num : 1
spv_proportion : 100
new_vocab : True
teacher_force : 100
beam_search : False
beam_size : 10
sampling : False
use_positional_embedding : False
unfrz_attn_epoch : 0
skip_unsup : False
truncated : False
pretrain : False

INFO:root:6865 known embedding. old mean: 0.000030 new mean -0.020280, old std 1.001408 new std 0.671768
INFO:root:loss:4.77515983581543 pr_loss:0.00031736784148961306 m_loss:4.774842262268066 grad:0.5829075574874878
INFO:root:Traning time: 93.91367411613464
INFO:root:avg training loss in epoch 0 sup:6.588739
INFO:root:validation loss in epoch 0 sup:4.612085 unsup:0.000000
INFO:root:time for epoch 0: 122.309070
INFO:root:saving model...
INFO:root:loss:4.079056739807129 pr_loss:0.002720771823078394 m_loss:4.076335906982422 grad:0.6724420189857483
INFO:root:Traning time: 182.33489322662354
INFO:root:avg training loss in epoch 1 sup:4.283855
INFO:root:validation loss in epoch 1 sup:4.267563 unsup:0.000000
INFO:root:time for epoch 1: 114.115848
INFO:root:saving model...
INFO:root:loss:3.7536509037017822 pr_loss:0.0018925225595012307 m_loss:3.751758337020874 grad:0.5897055268287659
INFO:root:Traning time: 275.126745223999
INFO:root:avg training loss in epoch 2 sup:3.884885
